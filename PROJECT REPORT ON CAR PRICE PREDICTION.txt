PROJECT REPORT ON CAR PRICE PREDICTION
With the covid 19 impact in the market, we have seen lot of changes in the car market. Now some cars are in demand hence making them costly and some are not in demand hence cheaper. One of our clients works with small traders, who sell used cars. With the change in market due to covid 19 impact, our client is facing problems with their previous car price valuation machine learning models. So, they are looking for new machine learning models from new data. We have to make car price valuation model.
FEATURE DETAILS:

CAR_NAME: Name and Brand of car
CAR_PRICES IN RUPEES: Target variable which told car price in market.
KMS_DRIVEN: The number of kilometer car has drived.
FUEL_TYPE: Which type of fuel used by car.
TRANSMISSION: Type of transmission i.e. the medium that transmits power generated by the engine to the wheels via a mechanical system of gears and gear trains.
OWNERSHIP: Who is the owner i.e. car is first hand,second hand or third etc.
MANUFACTURE: In which year car is manufactured.
ENGINE: Tells about power of engine.
SEATS: How many seats in the car.
IMPORTING REQUIRED LIBRARIES:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Lasso
from sklearn import metrics
READ THE DATA
df=pd.read_csv(r"C:\Users\OM RAJ PANDEY\Desktop\archive (1)\car_price.csv")
SEE FIRST FIVE ROWS
df.head()
CHECK SHAPE
df.shape
SEE INFORMATION
df.info()
#checking the number of missing values
df.isnull().sum()
SEE THE STATISTICS INFORMATION ABOUT DATASET
df.describe(include='all')
DO LABEL ENCODING ON OBJECT DATA TYPE FOR CHANGE IT INTO NUMERICAL DATA TYPE
from sklearn.preprocessing import LabelEncoder
le1=LabelEncoder()
df['car_name']=le1.fit_transform(df['car_name'])
le2=LabelEncoder()
df['car_prices_in_rupee']=le2.fit_transform(df['car_prices_in_rupee'])
le3=LabelEncoder()
df['kms_driven']=le3.fit_transform(df['kms_driven'])
le4=LabelEncoder()
df['fuel_type']=le4.fit_transform(df['fuel_type'])
le5=LabelEncoder()
df['transmission']=le5.fit_transform(df['transmission'])
le6=LabelEncoder()
df['ownership']=le6.fit_transform(df['ownership'])
le7=LabelEncoder()
df['engine']=le7.fit_transform(df['engine'])
le8=LabelEncoder()
df['Seats']=le8.fit_transform(df['Seats'])
SEE FIRST FIVE ROWS
df.head()
CREATE HEATMAP TO SEE THE CORRELATION
plt.figure(figsize=(10,7))
sns.heatmap(df.corr(), annot=True)
plt.title('Correlation between the columns')
plt.show()
CREATE BOXPLOT TO SEE THE OUTLIERS
df.plot(kind='box',figsize=(12,12),layout=(6,6),sharex=False,subplots=True)
CREATE BARPLOT TO SEE THE RELATIONSHIP BETWEEN CAR PRICE AND MANUFACTURE COLUMN
plt.figure(figsize=(10,6))
sns.barplot(x='manufacture',y='car_prices_in_rupee',data=df)
CHECK THE DATA SKEWNESS
Df2.skew()
REMOVE THE SKEWNESS BY np.log METHOD
np.log(df2['car_name'])
np.log(df2['kms_driven'])
np.log(df2['fuel_type'])
np.log(df2['transmission'])
np.log(df2['ownership'])
np.log(df2['manufacture'])
np.log(df2['engine'])
np.log(df2['Seats'])
MODEL BUILDING
#separating the data and label
x=df2.drop(columns=['Unnamed: 0','car_prices_in_rupee','manufacture'],axis=1)
y=df2['car_prices_in_rupee']
APPLY STANDARD SCALER ON X
SEE THE DATA OF X AND Y
print(x)
print(y)
# Train-Test split
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)
# Import the models
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
# Model Training
lr=LinearRegression()
lr.fit(x_train,y_train)
svm=SVR()
svm.fit(x_train,y_train)
rf=RandomForestRegressor()
rf.fit(x_train,y_train)
gr=GradientBoostingRegressor()
gr.fit(x_train,y_train)
#Prediction on Test Data
y_pred1=lr.predict(x_test)
y_pred2=svm.predict(x_test)
y_pred3=rf.predict(x_test)
y_pred4=gr.predict(x_test)
df3=pd.DataFrame({'Actual':y_test, 'lr':y_pred1, 'svm':y_pred2, 'rf':y_pred3, 'gr':y_pred4})
df3
COMPARE PERFORMANCE VISUALLY
plt.subplot(221)
plt.plot(df3['Actual'].iloc[0:11], label='Actual')
plt.plot(df3['lr'].iloc[0:11], label='lr')
plt.legend()

plt.subplot(222)
plt.plot(df3['Actual'].iloc[0:11], label='Actual')
plt.plot(df3['svm'].iloc[0:11], label="svm")
plt.legend()

plt.subplot(223)
plt.plot(df3['Actual'].iloc[0:11], label='Actual')
plt.plot(df3['rf'].iloc[0:11], label="rf")
plt.legend()

plt.subplot(224)
plt.plot(df3['Actual'].iloc[0:11], label='Actual')
plt.plot(df3['gr'].iloc[0:11], label="gr")
plt.tight_layout()
plt.legend()
#Evauating the Algorithm
score1=metrics.r2_score(y_test,y_pred1)
score2=metrics.r2_score(y_test,y_pred2)
score3=metrics.r2_score(y_test,y_pred3)
score4=metrics.r2_score(y_test,y_pred4)
ON THE BASIS OF R2 SCORE RANDOM FOREST IS OUR BEST MODEL
CALCULATE MEAN ABSOLUTE ERROR
s1=metrics.mean_absolute_error(y_test,y_pred1)
s2=metrics.mean_absolute_error(y_test,y_pred2)
s3=metrics.mean_absolute_error(y_test,y_pred3)
s4=metrics.mean_absolute_error(y_test,y_pred4)
CALCULATE MEAN SQUARRED ERROR
#mean_squared_error
t1=metrics.mean_squared_error(y_test,y_pred1)
t2=metrics.mean_squared_error(y_test,y_pred2)
t3=metrics.mean_squared_error(y_test,y_pred3)
t4=metrics.mean_squared_error(y_test,y_pred4)
from sklearn.linear_model import Ridge,Lasso,RidgeCV,LassoCV
lasscv=LassoCV(alphas=None, max_iter=100)
lasscv.fit(x_train,y_train)
alpha=lasscv.alpha_
alpha
lasso_reg=Lasso(alpha)
lasso_reg.fit(x_train,y_train)
lasso_reg.score(x_test,y_test)
#Using Ridge Regression model
ridgecv=RidgeCV(alphas=np.arange(0.001,0.1,0.01))
ridgecv.fit(x_train,y_train)
ridgecv.alpha_
ridge_model=Ridge(alpha=ridgecv.alpha_)
ridge_model.fit(x_train,y_train)
ridge_model.score(x_test,y_test)
#hyperparameter tuning
grid_param={'criterion':['friedman_mse'],'max_depth':range(10,15),'min_samples_leaf':range(2,6),'min_samples_split':range(3,8),'max_leaf_nodes':range(5,10)}
grid_search=GridSearchCV(estimator=gr,param_grid=grid_param,cv=5,n_jobs=-1)
grid_search.fit(x_train,y_train)
best_parameters=grid_search.best_params_
print(best_parameters)
gr=GradientBoostingRegressor(criterion= 'friedman_mse', max_depth= 10, max_leaf_nodes= 9, min_samples_leaf= 5, min_samples_split= 4)
gr.fit(x_train,y_train)
y_pred=gr.predict(x_test)
score=metrics.r2_score(y_test,y_pred)
print(score)
#cross validation score
c1=cross_val_score(lr,x_test,y_test,cv=4)
c2=cross_val_score(svm,x_test,y_test,cv=4)
c3=cross_val_score(rf,x_test,y_test,cv=4)
c4=cross_val_score(gr,x_test,y_test,cv=4)
print("Linear Regression model accuracy is:{}".format(c1.mean()*100))
print("SVR model accuracy is:{}".format(c2.mean()*100))
print("RandomForestRegressor model accuracy is:{}".format(c3.mean()*100))
print("GradientBoostingRegressor model accuracy is:{}".format(c4.mean()*100))
CHECK COLUMN NAMES FOR PREDICTION
df2.columns
df2.tail()
MAKING A NEW DATAFRAME
data={'car_name':720,'kms_driven':2440,'fuel_type':4,'transmission':0,'ownership':2
     ,'engine':61,'Seats':2}
df5=pd.DataFrame(data, index=[0])
df5
PREDICT CAR PRICE ON THE NEW DATA
new_pred=gr.predict(df5)
print(new_pred)
SAVING THE MODEL
#Save Model Using Joblib
gr=GradientBoostingRegressor()
gr.fit(x,y)
import joblib
joblib.dump(gr, 'model_joblib_gr')
model=joblib.load('model_joblib_gr')
model.predict(df5)
CONCLUSION:
RANDOM FOREST IS OUR BEST MODEL.
